{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Methods Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'I recommend that we engage in an arms race with Red.'\n",
    "split1 = sentence1.split(' ')\n",
    "sentence2 = 'I recommend that we don\\'t engage in an arms race with Red.'\n",
    "split2 = sentence2.split(' ')\n",
    "sentence3 = 'Red is who I recommend that we engage in an arms race with.'\n",
    "split3 = sentence3.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METEOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arize.com/glossary/meteor-score/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import single_meteor_score as score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I recommend that we engage in an arms race with Red.\" and \"I recommend that we don't engage in an arms race with Red.\" have a score of 0.9880128061946244\n",
      "\"I recommend that we engage in an arms race with Red.\" and \"Red is who I recommend that we engage in an arms race with.\" have a score of 0.8030202821869489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aryanshrivastava/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "score1 = score(split1, split2)\n",
    "score2 = score(split1, split3)\n",
    "\n",
    "print(f'\"{sentence1}\" and \"{sentence2}\" have a score of {score1}')\n",
    "print(f'\"{sentence1}\" and \"{sentence3}\" have a score of {score2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes on METEOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Semantically opposite phrases have high similarity score\n",
    "- Semantically similar phrases have low(er) similarity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEURT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/google-research/bleurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleurt import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /Users/aryanshrivastava/bleurt/bleurt/test_checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /Users/aryanshrivastava/bleurt/bleurt/test_checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:dbleurt_tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:dbleurt_tiny\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = '/Users/aryanshrivastava/bleurt/bleurt/test_checkpoint'\n",
    "scorer = score.BleurtScorer(checkpoint)\n",
    "score1 = scorer.score(references=[sentence1], candidates=[sentence2])[0]\n",
    "score2 = scorer.score(references=[sentence1], candidates=[sentence3])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I recommend that we engage in an arms race with Red.\" and \"I recommend that we don't engage in an arms race with Red.\" have a score of 0.5444111227989197\n",
      "\"I recommend that we engage in an arms race with Red.\" and \"Red is who I recommend that we engage in an arms race with.\" have a score of 0.7867673635482788\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{sentence1}\" and \"{sentence2}\" have a score of {score1}')\n",
    "print(f'\"{sentence1}\" and \"{sentence3}\" have a score of {score2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes on BLEURT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- semantically opposite phrases have low(er) score\n",
    "- semantically similar phrases have high(er) score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5475f8ae35af4c80b8ab32e9120ebea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577baef1c8cc4d469b8b526223cfb1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.34 seconds, 2.94 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c85348885b84a38af54cb25448ac11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833d89d41792401a861f2db0f7994ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.29 seconds, 3.49 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ref = [sentence1]\n",
    "cand1 = [sentence2]\n",
    "cand2 = [sentence3]\n",
    "\n",
    "F1_1 = score(cand1, ref, lang='en', verbose=True)[2]\n",
    "F1_2 = score(cand2, ref, lang='en', verbose=True)[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I recommend that we engage in an arms race with Red.\" and \"I recommend that we don't engage in an arms race with Red.\" have a score of 0.9750765562057495\n",
      "\"I recommend that we engage in an arms race with Red.\" and \"Red is who I recommend that we engage in an arms race with.\" have a score of 0.9442029595375061\n",
      "tensor(0.9272)\n",
      "tensor(0.9389)\n",
      "tensor(0.9330)\n",
      "tensor(0.9448)\n",
      "tensor(0.9448)\n",
      "tensor(0.9448)\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{sentence1}\" and \"{sentence2}\" have a score of {F1_1[0]}')\n",
    "print(f'\"{sentence1}\" and \"{sentence3}\" have a score of {F1_2[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes on BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- semantically opposite have really high score\n",
    "- semantically similar, but different structure has still high, but lower, score\n",
    "- takes pretty long to run the first time (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/potsawee/mqag0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selfcheckgpt.modeling_mqag import MQAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MQAG (race) initialized to cpu\n"
     ]
    }
   ],
   "source": [
    "mqag_model = MQAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = mqag_model.score(candidate=sentence2, reference=sentence1, num_questions=1, verbose=True)\n",
    "score2 = mqag_model.score(candidate=sentence3, reference=sentence1, num_questions=1, verbose=True)\n",
    "\n",
    "## note that MQAG is definitely not good for small examples like this, I just included this here for completeness\n",
    "## THIS TAKES INSANELY LONG TO RUN WITHOUT COMPUTE, DO NOT TRY AND RUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing global attention on multiple choice...\n",
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n",
      "Initialized Answering\n",
      "Q1: Why can't we engage in an arms race with Red?\n",
      "(1) [P(.|cand)=23.90%]\t[P(.|ref)=25.37%]\tBecause red has an army that is much bigger than white.\n",
      "(2) [P(.|cand)=47.58%]\t[P(.|ref)=45.06%]\tBecause the Army of Red is much bigger than White's.\n",
      "(3) [P(.|cand)=23.90%]\t[P(.|ref)=25.37%]\tBecause red has an army that is much bigger than white.\n",
      "(4) [P(.|cand)=4.62%]\t[P(.|ref)=4.20%]\tBecause white is already in an arms race with red.\n"
     ]
    }
   ],
   "source": [
    "# what the cell output would be for score1 code running\n",
    "\n",
    "example_output = '''Initializing global attention on multiple choice...\n",
    "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n",
    "Initialized Answering\n",
    "Q1: Why can't we engage in an arms race with Red?\n",
    "(1) [P(.|cand)=23.90%]\t[P(.|ref)=25.37%]\tBecause red has an army that is much bigger than white.\n",
    "(2) [P(.|cand)=47.58%]\t[P(.|ref)=45.06%]\tBecause the Army of Red is much bigger than White's.\n",
    "(3) [P(.|cand)=23.90%]\t[P(.|ref)=25.37%]\tBecause red has an army that is much bigger than white.\n",
    "(4) [P(.|cand)=4.62%]\t[P(.|ref)=4.20%]\tBecause white is already in an arms race with red.'''\n",
    "\n",
    "print(example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_scores = {'kl_div': 0.0017558218082514964, 'counting': 0.0, 'hellinger': 0.0006205150575312447, 'total_variation': 0.02520403265953064}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes on MQAG (Multiple-choice Question Answering and Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- idk, super long to run\n",
    "- not good for small texts, good for larger texts\n",
    "- I'm pretty sure lower scores correspond to higher similarity\n",
    "    - so, semantically opposite sentences have very similar scores\n",
    "    - But, I think this is just a function of a bad question being generated due to length of the sentences\n",
    "    - DO NOT USE THIS METHOD FOR SHORT RESPONSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
